{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from random import sample\n",
    "from itertools import count\n",
    "from IPython import display\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    logging.info(f'Running on {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    actions = {\n",
    "        \"idle\" : 0,\n",
    "        \"left\" : 1,\n",
    "        \"right\" : 2,\n",
    "        \"up\" : 3,\n",
    "        \"down\" : 4\n",
    "    }\n",
    "\n",
    "    def __init__(self, size, num_obstacles, num_enemies):        \n",
    "        super(MyEnv, self).__init__()\n",
    "        self.size = size\n",
    "        self.num_obstacles = num_obstacles\n",
    "        self.num_enemies = num_enemies\n",
    "\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "        self.hidden = 0\n",
    "        self.passable = 1\n",
    "        self.obstacle = 2        \n",
    "        self.enemy = 3\n",
    "        self.exit = 4\n",
    "        self.agent = 5        \n",
    "\n",
    "        self.grid = None\n",
    "        self.mask = None\n",
    "        self.fov_count = None\n",
    "        self.agent_position = None\n",
    "        self.previous_positions = set()\n",
    "\n",
    "\n",
    "    def reset_mask(self):\n",
    "        self.mask = np.zeros((self.size+2, self.size+2))\n",
    "\n",
    "    def get_mask(self, reset = False):\n",
    "        if self.mask is None or reset:\n",
    "            self.reset_mask()\n",
    "        x = np.arange(0, 14)\n",
    "        y = np.arange(0, 14)\n",
    "        agent_y, agent_x  = self.agent_position[0], self.agent_position[1]\n",
    "        fov = (x[np.newaxis,:]-agent_x)**2 + (y[:,np.newaxis]-agent_y)**2 < 2**2\n",
    "        self.mask[fov] = 1\n",
    "\n",
    "        return self.mask  \n",
    "\n",
    "    def get_fov_count(self):\n",
    "        self.fov_count = np.unique(self.mask, return_counts=True)[1][0]\n",
    "\n",
    "        return self.fov_count\n",
    "\n",
    "    def reset(self):\n",
    "        #generates grid\n",
    "        self.grid = np.ones((self.size+2, self.size+2))*2\n",
    "        self.grid[1:-1, 1:-1] = self.passable\n",
    "        #generates obstacles\n",
    "        for i in range(self.num_obstacles):\n",
    "            self.grid[(random.randint(1,self.size), random.randint(1,self.size))] = self.obstacle\n",
    "        #generates enemies\n",
    "        for i in range(self.num_enemies):\n",
    "            self.grid[(random.randint(1,self.size), random.randint(1,self.size))] = self.enemy\n",
    "        #generates exit\n",
    "        self.grid[(random.randint(1,self.size), random.randint(1,self.size))] = self.exit\n",
    "        #generates agent position\n",
    "        self.previous_positions = set()\n",
    "        self.agent_position = (random.randint(1,self.size), random.randint(1,self.size))        \n",
    "        self.grid[self.agent_position] = self.agent\n",
    "        self.previous_positions.add(self.agent_position)\n",
    "        \n",
    "        self.mask = self.get_mask(reset = True)\n",
    "        self.fov_count = self.get_fov_count()        \n",
    "\n",
    "        return self.grid * self.mask\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        fov_count = self.fov_count\n",
    "        \n",
    "        agent_y, agent_x = self.agent_position[0], self.agent_position[1]               \n",
    "\n",
    "        if action == self.actions[\"idle\"]:\n",
    "            reward -= 0.01\n",
    "            pass\n",
    "        elif action == self.actions[\"left\"]:\n",
    "            agent_y, agent_x = agent_y, agent_x-1            \n",
    "        elif action == self.actions[\"right\"]:\n",
    "            agent_y, agent_x = agent_y, agent_x+1                     \n",
    "        elif action == self.actions[\"up\"]:\n",
    "            agent_y, agent_x = agent_y-1, agent_x                       \n",
    "        elif action == self.actions[\"down\"]:\n",
    "            agent_y, agent_x = agent_y+1, agent_x         \n",
    "\n",
    "        agent_position_new = (agent_y, agent_x)\n",
    "\n",
    "        if self.grid[agent_position_new] == self.obstacle:\n",
    "            reward -= 0.02\n",
    "            agent_position_new = self.agent_position\n",
    "        elif self.grid[agent_position_new] == self.enemy:\n",
    "            agent_position_new = self.agent_position            \n",
    "            reward -= 1\n",
    "            done = True\n",
    "        elif self.grid[agent_position_new] == self.exit:\n",
    "            reward += 1\n",
    "            done = True\n",
    "        elif self.grid[agent_position_new] == self.passable:\n",
    "            reward += 0.01\n",
    "            self.grid[self.agent_position] = self.passable\n",
    "            self.grid[agent_position_new] = self.agent\n",
    "        \n",
    "        self.agent_position = agent_position_new        \n",
    "        if self.agent_position in self.previous_positions:\n",
    "            reward -= 0.03\n",
    "\n",
    "        self.previous_positions.add(self.agent_position)\n",
    "        self.mask = self.get_mask() \n",
    "        self.fov_count = self.get_fov_count()\n",
    "        reward += (0.01 * (fov_count - self.fov_count))\n",
    "\n",
    "        return self.grid * self.mask, reward, done, {}   \n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.grid * self.mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 2., 5., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "env = MyEnv(size=12, num_obstacles=16, num_enemies=2)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-359-57769437286a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msample_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0msample_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "sample_state = env.reset()\n",
    "sample_img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for j in range(200):    \n",
    "    sleep(5)\n",
    "    sample_action = env.action_space.sample()    \n",
    "    state, reward, done, _ = env.step(sample_action)\n",
    "    print(f\"Sample action: {sample_action}, {list(env.actions.keys())[list(env.actions.values()).index(sample_action)]}, Reward: {reward}, Done: {done}\", end = \"\")\n",
    "    sample_img.set_data(env.render(mode='rbg_array'))\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        super(QNetwork,self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, 8, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 4, 1)\n",
    "        self.conv3 = nn.Conv2d(16, 8, 2, 1)\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.fc1= nn.Linear(72, 36)        \n",
    "        self.fc2= nn.Linear(36, 18)\n",
    "        self.fc3= nn.Linear(18, 8)         \n",
    "        self.fc4 = nn.Linear(8, action_size)\n",
    "\n",
    "    def mish(self, x):\n",
    "        return x*(torch.tanh(F.softplus(x)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)    \n",
    "        x = self.mish(self.conv1(x))    \n",
    "        x = self.mish(self.conv2(x))\n",
    "        x = self.mish(self.conv3(x))\n",
    "        #print(x.size()) \n",
    "        x = self.flatten(x)\n",
    "        x = self.mish(self.fc1(x))        \n",
    "        x = self.mish(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        \n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class QNetwork(nn.Module):\n",
    "#     def __init__(self, state_size, action_size, seed):\n",
    "#         super(QNetwork,self).__init__()\n",
    "#         self.seed = torch.manual_seed(seed)\n",
    "\n",
    "#         self.flatten = nn.Flatten(start_dim=1)\n",
    "#         self.fc1= nn.Linear(state_size, 128)        \n",
    "#         self.fc2= nn.Linear(128, action_size)\n",
    "\n",
    "#     def mish(self, x):\n",
    "#         return x*(torch.tanh(F.softplus(x)))\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = x.unsqueeze(1)\n",
    "#         #print(x.size()) \n",
    "#         x = self.flatten(x)\n",
    "#         x = self.mish(self.fc1(x))\n",
    "        \n",
    "#         return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque \n",
    "\n",
    "buffer_size = 10000\n",
    "batch_size = 1024\n",
    "gamma = 0.99\n",
    "tau = 0.0001\n",
    "learning_rate = 0.0001\n",
    "update_rate = 4\n",
    "\n",
    "class Agent():    \n",
    "    def __init__(self, state_size, action_size, seed):       \n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)        \n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr = learning_rate)\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.experiences = namedtuple(\"Experience\", field_names=[\"state\",\n",
    "                                                               \"action\",\n",
    "                                                               \"reward\",\n",
    "                                                               \"next_state\",\n",
    "                                                               \"done\"])\n",
    "        self.t_step = 0\n",
    "        self.total_loss = []\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.append(self.experiences(state, action, reward, next_state, done))\n",
    "        self.t_step = (self.t_step+1) % update_rate\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > batch_size:\n",
    "                #experience = random.sample(self.memory, batch_size)\n",
    "                experience = [self.memory.popleft() for item in range(batch_size)]\n",
    "                self.learn(experience, gamma)\n",
    "\n",
    "    def act(self, state, eps = 0):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "            \n",
    "    def learn(self, experience, gamma):\n",
    "\n",
    "        states = torch.from_numpy(np.stack([e.state for e in experience if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experience if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experience if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in experience if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experience if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        #criterion = torch.nn.SmoothL1Loss()\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        self.qnetwork_local.train()\n",
    "        self.qnetwork_target.eval()\n",
    "               \n",
    "        predicted_targets = self.qnetwork_local(states).gather(1,actions)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            labels_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        labels = rewards + (gamma*labels_next*(1-dones))\n",
    "        \n",
    "        loss = criterion(predicted_targets, labels).to(device)\n",
    "        self.total_loss.append(loss.item())\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.soft_update(self.qnetwork_local,self.qnetwork_target, tau)\n",
    "            \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(),\n",
    "                                           local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=196, action_size=5, seed=55555)\n",
    "\n",
    "def training_loop(n_episodes = 5000000, max_t = 100, eps_start = 1, eps_end = 0.01,\n",
    "       eps_decay = 0.995):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            scores_window.append(score)\n",
    "            scores.append(score)\n",
    "            if done:\n",
    "                break            \n",
    "            \n",
    "        eps = max(eps*eps_decay, eps_end)\n",
    "        print(f\"\\rEpisode {i_episode}\\tEpisode Score {score}\\tLast iters score {np.mean(scores_window)}\", end=\"\")        \n",
    "        if i_episode % 1000==0:\n",
    "            print(f\"\\rEpisode {i_episode}\\tAverage Score {np.mean(scores)}\\nAverage loss: {np.mean(agent.total_loss)}\\tRandom action sample probability: {eps}\\n\")\n",
    "            agent.total_loss = []\n",
    "            \n",
    "        if np.mean(scores_window) >= 1:\n",
    "            print(f\"\\nEnvironment solved in {i_episode-100} epsiodes.\\tAverage score: {np.mean(scores_window)}\")\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break        \n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode 100\tAverage Score -0.7987949478025521\n",
      "Average loss: 0.028834895629967962\tRandom action sample probability: 0.6057704364907278\n",
      "\n",
      "Episode 200\tAverage Score -0.9023618696811893\n",
      "Average loss: 0.017730641993694007\tRandom action sample probability: 0.3669578217261671\n",
      "\n",
      "Episode 300\tAverage Score -0.9828841597738894\n",
      "Average loss: 0.012997246347367764\tRandom action sample probability: 0.22229219984074702\n",
      "\n",
      "Episode 400\tAverage Score -1.0861027414176343\n",
      "Average loss: 0.010586126940324903\tRandom action sample probability: 0.1346580429260134\n",
      "\n",
      "Episode 500\tAverage Score -1.164526120571305\n",
      "Average loss: 0.006029776918391387\tRandom action sample probability: 0.08157186144027828\n",
      "\n",
      "Episode 600\tAverage Score -1.2541280379371664\n",
      "Average loss: 0.005134648203642832\tRandom action sample probability: 0.0494138221100385\n",
      "\n",
      "Episode 700\tAverage Score -1.307130503144654\n",
      "Average loss: 0.004549811076786783\tRandom action sample probability: 0.029933432588273214\n",
      "\n",
      "Episode 800\tAverage Score -1.3719859633890459\n",
      "Average loss: 0.0030303909184618127\tRandom action sample probability: 0.018132788524664028\n",
      "\n",
      "Episode 900\tAverage Score -1.4233673742248483\n",
      "Average loss: 0.0025012414747228227\tRandom action sample probability: 0.01098430721937979\n",
      "\n",
      "Episode 1000\tAverage Score -1.4756522134041807\n",
      "Average loss: 0.0027517988113686443\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 1100\tAverage Score -1.512418463239359\n",
      "Average loss: 0.0021531627733363872\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 1200\tAverage Score -1.5410872868636207\n",
      "Average loss: 0.0018391138227242562\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 1300\tAverage Score -1.5646683235542802\n",
      "Average loss: 0.002037622340139933\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 1400\tAverage Score -1.5900102525276423\n",
      "Average loss: 0.0014615063555538654\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 1500\tAverage Score -1.6131016942923295\n",
      "Average loss: 0.002225623343191627\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 1600\tAverage Score -1.6270082566351636\n",
      "Average loss: 0.0022110221441835167\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 1700\tAverage Score -1.6374810948226912\n",
      "Average loss: 0.002067629998135898\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 1800\tAverage Score -1.644628480993395\n",
      "Average loss: 0.002392519588789178\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 1900\tAverage Score -1.6508301543737303\n",
      "Average loss: 0.0021165299436284434\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 2000\tAverage Score -1.6534596149856406\n",
      "Average loss: 0.002293554587393171\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 2100\tAverage Score -1.661463384264183\n",
      "Average loss: 0.001872127829119563\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 2200\tAverage Score -1.664612966057286\n",
      "Average loss: 0.0021084947058827514\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 2300\tAverage Score -1.6708351018826184\n",
      "Average loss: 0.002606150487230884\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 2400\tAverage Score -1.6763961488044115\n",
      "Average loss: 0.0014028449601028114\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 2500\tAverage Score -1.6797265846695053\n",
      "Average loss: 0.0018569195009250608\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 2600\tAverage Score -1.6858559713074335\n",
      "Average loss: 0.001819335719725738\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 2700\tAverage Score -1.68341528513063\n",
      "Average loss: 0.0018887737154727803\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 2800\tAverage Score -1.6894486626834158\n",
      "Average loss: 0.0014314592537832344\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 2900\tAverage Score -1.6898879614254454\n",
      "Average loss: 0.002372420955604563\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 3000\tAverage Score -1.6912290534846603\n",
      "Average loss: 0.0013900675519835203\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 3100\tAverage Score -1.692520888420658\n",
      "Average loss: 0.0017871121203319894\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 3200\tAverage Score -1.6953320688844868\n",
      "Average loss: 0.002723828798884319\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 3300\tAverage Score -1.6979358160746052\n",
      "Average loss: 0.0014669419615529478\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 3400\tAverage Score -1.699538524680736\n",
      "Average loss: 0.002219630874201862\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 3500\tAverage Score -1.7035692845426247\n",
      "Average loss: 0.0020108626663891804\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 3600\tAverage Score -1.7029527224854863\n",
      "Average loss: 0.0016944771115150717\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 3700\tAverage Score -1.7058504773079533\n",
      "Average loss: 0.001700312620960176\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 3800\tAverage Score -1.7031353194898535\n",
      "Average loss: 0.0022543936243487727\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 3900\tAverage Score -1.705591631186947\n",
      "Average loss: 0.0021013246991464663\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 4000\tAverage Score -1.7067391228545439\n",
      "Average loss: 0.001881197599383692\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 4100\tAverage Score -1.7116605672820104\n",
      "Average loss: 0.0016351958620361984\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 4200\tAverage Score -1.7111150723875772\n",
      "Average loss: 0.0016686082558913364\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 4300\tAverage Score -1.7131310674172684\n",
      "Average loss: 0.0011426106706494466\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 4400\tAverage Score -1.7137280758279083\n",
      "Average loss: 0.0017016242491081357\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 4500\tAverage Score -1.714277328199826\n",
      "Average loss: 0.0019702824970914256\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 4600\tAverage Score -1.7165290131398656\n",
      "Average loss: 0.001282421694486402\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 4700\tAverage Score -1.7158159515894094\n",
      "Average loss: 0.0021072563767019245\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 4800\tAverage Score -1.7156033183174497\n",
      "Average loss: 0.0017255466373171657\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 4900\tAverage Score -1.7162345687081941\n",
      "Average loss: 0.0018176867096270951\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 5000\tAverage Score -1.7192633217476594\n",
      "Average loss: 0.0016883777635585931\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 5100\tAverage Score -1.7221648181535198\n",
      "Average loss: 0.0012416440498782323\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 5200\tAverage Score -1.721167338362863\n",
      "Average loss: 0.001915984321385622\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 5300\tAverage Score -1.7224442137100413\n",
      "Average loss: 0.001933062559692189\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 5400\tAverage Score -1.722927334181613\n",
      "Average loss: 0.001334145775116566\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 5500\tAverage Score -1.7237562222703167\n",
      "Average loss: 0.0013739595364313574\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 5600\tAverage Score -1.7252872664619865\n",
      "Average loss: 0.0012932719287669493\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 5700\tAverage Score -1.7249443011650352\n",
      "Average loss: 0.0015383143618237227\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 5800\tAverage Score -1.7236776313380924\n",
      "Average loss: 0.001578063387165053\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 5900\tAverage Score -1.724458881155864\n",
      "Average loss: 0.0015240788019986616\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 6000\tAverage Score -1.725358113352321\n",
      "Average loss: 0.001178610511124134\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 6100\tAverage Score -1.7264197691844954\n",
      "Average loss: 0.0013707424449320468\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 6200\tAverage Score -1.7269054711734784\n",
      "Average loss: 0.001897983386879787\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 6300\tAverage Score -1.7261573078695989\n",
      "Average loss: 0.0014084211175536944\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 6400\tAverage Score -1.7269428361036872\n",
      "Average loss: 0.0009186477691400796\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 6500\tAverage Score -1.727186010393936\n",
      "Average loss: 0.002064047801670515\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 6600\tAverage Score -1.7276406793796406\n",
      "Average loss: 0.0013454364897269341\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 6700\tAverage Score -1.7278367744861176\n",
      "Average loss: 0.0015608858317136764\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 6800\tAverage Score -1.726543351953045\n",
      "Average loss: 0.002252714365669009\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 6900\tAverage Score -1.7264151091035735\n",
      "Average loss: 0.0011798714080618487\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 7000\tAverage Score -1.725913695946508\n",
      "Average loss: 0.0019374391025242705\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 7100\tAverage Score -1.725599781027233\n",
      "Average loss: 0.0017765807828658985\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 7200\tAverage Score -1.7276057328476515\n",
      "Average loss: 0.0017761030855278175\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 7300\tAverage Score -1.7280536687886447\n",
      "Average loss: 0.0014304324431577697\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 7400\tAverage Score -1.7284508158154794\n",
      "Average loss: 0.001080099548239054\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 7500\tAverage Score -1.7273044690766315\n",
      "Average loss: 0.0019659401926522455\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 7600\tAverage Score -1.726808785943604\n",
      "Average loss: 0.0016137337894178926\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 7700\tAverage Score -1.7252871005123285\n",
      "Average loss: 0.0018314959403748314\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 7800\tAverage Score -1.7256170639899622\n",
      "Average loss: 0.001201947116189533\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 7900\tAverage Score -1.7234971790141143\n",
      "Average loss: 0.0012830627674702555\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 8000\tAverage Score -1.722647325511925\n",
      "Average loss: 0.0013367404915495878\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 8100\tAverage Score -1.7224019787731135\n",
      "Average loss: 0.0017632289543851381\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 8200\tAverage Score -1.7224281343830286\n",
      "Average loss: 0.00126192236202769\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 8300\tAverage Score -1.7224603230225222\n",
      "Average loss: 0.0017346573230396542\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 8400\tAverage Score -1.7214239942816223\n",
      "Average loss: 0.0009900457022013142\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 8500\tAverage Score -1.7204596405169852\n",
      "Average loss: 0.0008748709321177254\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 8600\tAverage Score -1.72034239724765\n",
      "Average loss: 0.0013284643452304106\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 8700\tAverage Score -1.721108345162822\n",
      "Average loss: 0.0014331609854707494\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 8800\tAverage Score -1.7210957172794725\n",
      "Average loss: 0.0011655046125977403\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 8900\tAverage Score -1.721124938719453\n",
      "Average loss: 0.001835168740298185\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 9000\tAverage Score -1.7190878806268686\n",
      "Average loss: 0.001593368089136978\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 9100\tAverage Score -1.7185179235702057\n",
      "Average loss: 0.0013561600702814759\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 9200\tAverage Score -1.718200647524041\n",
      "Average loss: 0.001479382212791178\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 9300\tAverage Score -1.7191871969270445\n",
      "Average loss: 0.0015278108679922298\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 9400\tAverage Score -1.7197010154890446\n",
      "Average loss: 0.0022419191146683362\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 9500\tAverage Score -1.7193155844516683\n",
      "Average loss: 0.0015520306058331495\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 9600\tAverage Score -1.719367626311443\n",
      "Average loss: 0.0014302371069788932\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 9700\tAverage Score -1.7207370037002503\n",
      "Average loss: 0.0011335795150241917\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 9800\tAverage Score -1.7212115378192814\n",
      "Average loss: 0.001592498557228181\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 9900\tAverage Score -1.7210986663027403\n",
      "Average loss: 0.0012570653084872498\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 10000\tAverage Score -1.7198651816559234\n",
      "Average loss: 0.0015119212854187935\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 10100\tAverage Score -1.7187076712168454\n",
      "Average loss: 0.0012460182446779476\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 10200\tAverage Score -1.7180621371394407\n",
      "Average loss: 0.001125370358931832\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 10300\tAverage Score -1.7179036941219124\n",
      "Average loss: 0.001276113823728843\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 10400\tAverage Score -1.7184378207546787\n",
      "Average loss: 0.0012534469262593323\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 10500\tAverage Score -1.718128971607806\n",
      "Average loss: 0.0012450749549316243\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 10600\tAverage Score -1.7185012344156267\n",
      "Average loss: 0.001117317103004704\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 10700\tAverage Score -1.7189992925007043\n",
      "Average loss: 0.0011797767336247488\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 10800\tAverage Score -1.7189428991552327\n",
      "Average loss: 0.0016386433724417454\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 10900\tAverage Score -1.7187674803103135\n",
      "Average loss: 0.0016271527209836575\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 11000\tAverage Score -1.7176232783597267\n",
      "Average loss: 0.0016203651062419845\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 11100\tAverage Score -1.7166335760820617\n",
      "Average loss: 0.001263959786026842\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 11200\tAverage Score -1.7165973354521178\n",
      "Average loss: 0.001553683597012423\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 11300\tAverage Score -1.7153473024284203\n",
      "Average loss: 0.0013128954839582245\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 11400\tAverage Score -1.7149479721334746\n",
      "Average loss: 0.000862373350537382\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 11500\tAverage Score -1.7151781763413034\n",
      "Average loss: 0.0008783348409148554\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 11600\tAverage Score -1.715856393480736\n",
      "Average loss: 0.0012953168654348702\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 11700\tAverage Score -1.715927304846403\n",
      "Average loss: 0.0014903181840458678\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 11800\tAverage Score -1.7154707552222812\n",
      "Average loss: 0.0013502491961440279\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 11900\tAverage Score -1.7151934898813803\n",
      "Average loss: 0.0014033179148100316\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 12000\tAverage Score -1.715451302639686\n",
      "Average loss: 0.0012919985521067348\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 12100\tAverage Score -1.7154102593782334\n",
      "Average loss: 0.000965497994911857\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 12200\tAverage Score -1.715083147281456\n",
      "Average loss: 0.001020658845340626\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 12300\tAverage Score -1.7143960891244177\n",
      "Average loss: 0.001016352273290977\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 12400\tAverage Score -1.7130964385998861\n",
      "Average loss: 0.0013668584352773097\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 12500\tAverage Score -1.713021348169886\n",
      "Average loss: 0.0010680335544748232\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 12600\tAverage Score -1.712476855117922\n",
      "Average loss: 0.0009076363963281943\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 12700\tAverage Score -1.7116569234030428\n",
      "Average loss: 0.0008549819991458208\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 12800\tAverage Score -1.7106101485534801\n",
      "Average loss: 0.0007668240545576231\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 12900\tAverage Score -1.71096796777706\n",
      "Average loss: 0.0017628263565711677\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 13000\tAverage Score -1.7099857924566593\n",
      "Average loss: 0.0011751201754021975\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 13100\tAverage Score -1.7093176140238273\n",
      "Average loss: 0.0009997467102948576\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 13200\tAverage Score -1.709947299864487\n",
      "Average loss: 0.0013171735918149352\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 13300\tAverage Score -1.7086957861453347\n",
      "Average loss: 0.0010797430470120162\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 13400\tAverage Score -1.7081031730224867\n",
      "Average loss: 0.0011464221558223169\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 13500\tAverage Score -1.7080437361169698\n",
      "Average loss: 0.0015433330101788873\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 13600\tAverage Score -1.7077256229275661\n",
      "Average loss: 0.0012157598714111373\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 13700\tAverage Score -1.707292796454205\n",
      "Average loss: 0.0009732885389692254\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 13800\tAverage Score -1.7075725796877328\n",
      "Average loss: 0.0009082900447538123\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 13900\tAverage Score -1.707340151133958\n",
      "Average loss: 0.0011768871109880921\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 14000\tAverage Score -1.7071103641910317\n",
      "Average loss: 0.0008262605493655429\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 14100\tAverage Score -1.7074645881278185\n",
      "Average loss: 0.0009867197529981947\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 14200\tAverage Score -1.7072279655214737\n",
      "Average loss: 0.0012700917723122984\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 14300\tAverage Score -1.7067155597027803\n",
      "Average loss: 0.001181306061334908\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 14400\tAverage Score -1.7061834461883016\n",
      "Average loss: 0.0011935964317267968\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 14500\tAverage Score -1.705459132854417\n",
      "Average loss: 0.0011983050091657788\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 14600\tAverage Score -1.705287554289773\n",
      "Average loss: 0.0011399873409471992\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 14700\tAverage Score -1.7053638207028523\n",
      "Average loss: 0.0012257828299577038\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 14800\tAverage Score -1.7054597239408458\n",
      "Average loss: 0.0011608123604673892\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 14900\tAverage Score -1.7053665205411532\n",
      "Average loss: 0.0013637508511439795\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 15000\tAverage Score -1.7049603415308867\n",
      "Average loss: 0.0014719960357372959\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 15100\tAverage Score -1.7038069202834605\n",
      "Average loss: 0.0008922946988604962\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 15200\tAverage Score -1.7037982494287869\n",
      "Average loss: 0.0010778591030329051\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 15300\tAverage Score -1.7033616784087455\n",
      "Average loss: 0.0006551567174028605\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 15400\tAverage Score -1.70208634897105\n",
      "Average loss: 0.0014646579252762927\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 15500\tAverage Score -1.7013541640575236\n",
      "Average loss: 0.001565089549532988\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 15600\tAverage Score -1.7008835996876788\n",
      "Average loss: 0.0008294073166325688\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 15700\tAverage Score -1.700930056208114\n",
      "Average loss: 0.0007491841614763769\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 15800\tAverage Score -1.7005133351214412\n",
      "Average loss: 0.0012030145994180606\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 15900\tAverage Score -1.7000408566099903\n",
      "Average loss: 0.0012476450763642787\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 16000\tAverage Score -1.6994492327055573\n",
      "Average loss: 0.001160160405561328\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 16100\tAverage Score -1.6998549112376706\n",
      "Average loss: 0.0010656703205313533\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 16200\tAverage Score -1.6997019336065426\n",
      "Average loss: 0.0007340002501021243\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 16300\tAverage Score -1.6988048398944597\n",
      "Average loss: 0.0012528871913673357\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 16400\tAverage Score -1.6995207095901554\n",
      "Average loss: 0.0008986199940813498\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 16500\tAverage Score -1.6994839302310372\n",
      "Average loss: 0.000997756331344135\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 16600\tAverage Score -1.6989222437650395\n",
      "Average loss: 0.0016364644850707715\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 16700\tAverage Score -1.6985813402070278\n",
      "Average loss: 0.0009398718345134208\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 16800\tAverage Score -1.6983495199549727\n",
      "Average loss: 0.0015501641836534771\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 16900\tAverage Score -1.6979187030191778\n",
      "Average loss: 0.0009097490314161405\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 17000\tAverage Score -1.6976306203282738\n",
      "Average loss: 0.0011374450259609148\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 17100\tAverage Score -1.6970669739872464\n",
      "Average loss: 0.001173442488329278\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 17200\tAverage Score -1.696349700239882\n",
      "Average loss: 0.0010332286939956248\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 17300\tAverage Score -1.69663257211029\n",
      "Average loss: 0.0008223305178237044\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 17400\tAverage Score -1.6958614982232252\n",
      "Average loss: 0.0014547587618128294\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 17500\tAverage Score -1.695558613754112\n",
      "Average loss: 0.0011775888764532284\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 17600\tAverage Score -1.6952370012243039\n",
      "Average loss: 0.0012396787577826115\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 17700\tAverage Score -1.6953493878507553\n",
      "Average loss: 0.0009592816786607727\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 17800\tAverage Score -1.6950538527902117\n",
      "Average loss: 0.0013520665524993092\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 17900\tAverage Score -1.694197004296799\n",
      "Average loss: 0.0013911931326017818\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 18000\tAverage Score -1.6934186359954193\n",
      "Average loss: 0.0009924250858603044\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 18100\tAverage Score -1.6933060988311541\n",
      "Average loss: 0.0008519983352420644\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 18200\tAverage Score -1.6925584657375365\n",
      "Average loss: 0.0005758265207987279\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 18300\tAverage Score -1.6919882034251263\n",
      "Average loss: 0.0007929179215958962\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 18400\tAverage Score -1.6920503358282204\n",
      "Average loss: 0.0010279309703037144\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 18500\tAverage Score -1.691928847747059\n",
      "Average loss: 0.001167126737224559\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 18600\tAverage Score -1.6921101937864724\n",
      "Average loss: 0.0013406641600239608\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 18700\tAverage Score -1.6913561594375301\n",
      "Average loss: 0.0007204032503068447\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 18800\tAverage Score -1.691299575400036\n",
      "Average loss: 0.0006280166679061949\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 18900\tAverage Score -1.6905353254083704\n",
      "Average loss: 0.0009638749211767896\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 19000\tAverage Score -1.6899754631029513\n",
      "Average loss: 0.0010752751550171524\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 19100\tAverage Score -1.6894397439605908\n",
      "Average loss: 0.001033506899451216\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 19200\tAverage Score -1.6892753094082003\n",
      "Average loss: 0.0008592267899075523\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 19300\tAverage Score -1.688845483998437\n",
      "Average loss: 0.001014249289356586\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 19400\tAverage Score -1.6889183002500463\n",
      "Average loss: 0.001175353245344013\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 19500\tAverage Score -1.6886737789910655\n",
      "Average loss: 0.0013168064440833404\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 19600\tAverage Score -1.6880930485057124\n",
      "Average loss: 0.0012921847842840685\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 19700\tAverage Score -1.6872307544430394\n",
      "Average loss: 0.0010372117620944562\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 19800\tAverage Score -1.6862557669113871\n",
      "Average loss: 0.0008735636743949726\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 19900\tAverage Score -1.6855033988860322\n",
      "Average loss: 0.0013575165616607086\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 20000\tAverage Score -1.6851753252730748\n",
      "Average loss: 0.0010662916523870082\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 20100\tAverage Score -1.6849380468615427\n",
      "Average loss: 0.0010200314767037828\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 20200\tAverage Score -1.6840074651714232\n",
      "Average loss: 0.0009676661639888254\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 20300\tAverage Score -1.6837085119614048\n",
      "Average loss: 0.0009704119467642159\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 20400\tAverage Score -1.6831813929167385\n",
      "Average loss: 0.0007050451951929264\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 20500\tAverage Score -1.6822766418465442\n",
      "Average loss: 0.0007928921550046653\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 20600\tAverage Score -1.6823712150284162\n",
      "Average loss: 0.0011074442946765986\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 20700\tAverage Score -1.6822159380254142\n",
      "Average loss: 0.001214527673760636\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 20800\tAverage Score -1.6820680154175525\n",
      "Average loss: 0.0012247041187947616\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 20900\tAverage Score -1.6817554382853692\n",
      "Average loss: 0.0013660416847819255\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 21000\tAverage Score -1.6815878360543726\n",
      "Average loss: 0.0010069188332029928\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 21100\tAverage Score -1.6804944516510227\n",
      "Average loss: 0.0010573546023806557\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 21200\tAverage Score -1.679841674690048\n",
      "Average loss: 0.0006118811628160378\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 21300\tAverage Score -1.679141007837669\n",
      "Average loss: 0.0008191350643755868\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 21400\tAverage Score -1.6792159541219818\n",
      "Average loss: 0.00127088980581094\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 21500\tAverage Score -1.6792891079605383\n",
      "Average loss: 0.0011939864663872868\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 21600\tAverage Score -1.6784592428203238\n",
      "Average loss: 0.0006954300682991743\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 21700\tAverage Score -1.6774024139442367\n",
      "Average loss: 0.0008642697486922973\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 21800\tAverage Score -1.677163407436176\n",
      "Average loss: 0.0013619014385363294\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 21900\tAverage Score -1.676335468295596\n",
      "Average loss: 0.0010659172286977992\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 22000\tAverage Score -1.6763858884893286\n",
      "Average loss: 0.0005104159095531537\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 22100\tAverage Score -1.6758260291882108\n",
      "Average loss: 0.0007985881238710135\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 22200\tAverage Score -1.6755134019046118\n",
      "Average loss: 0.001442402782332566\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 22300\tAverage Score -1.6750260595569366\n",
      "Average loss: 0.0012168658560969764\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 22400\tAverage Score -1.6747845963020347\n",
      "Average loss: 0.0009048961277585476\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 22500\tAverage Score -1.6743669599692788\n",
      "Average loss: 0.0007919508789200336\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 22600\tAverage Score -1.6740081144229806\n",
      "Average loss: 0.0011682493075366234\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 22700\tAverage Score -1.67282375023762\n",
      "Average loss: 0.0012805635245361675\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 22800\tAverage Score -1.672564401967816\n",
      "Average loss: 0.001135403950077792\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 22900\tAverage Score -1.6721171319347088\n",
      "Average loss: 0.0008221632509957999\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 23000\tAverage Score -1.671465574054923\n",
      "Average loss: 0.0007546966429799795\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 23100\tAverage Score -1.671031962322142\n",
      "Average loss: 0.0012220357990978907\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 23200\tAverage Score -1.6702653222890094\n",
      "Average loss: 0.001443335534228633\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 23300\tAverage Score -1.669722010561841\n",
      "Average loss: 0.0014586540314161943\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 23400\tAverage Score -1.6692383185396087\n",
      "Average loss: 0.0012636568507231358\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 23500\tAverage Score -1.6685309191132824\n",
      "Average loss: 0.0009904427570290864\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 23600\tAverage Score -1.6681419207448687\n",
      "Average loss: 0.000718219791047482\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 23700\tAverage Score -1.667761503186163\n",
      "Average loss: 0.0015738839930337337\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 23800\tAverage Score -1.6671613413929545\n",
      "Average loss: 0.0008841653121635318\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 23900\tAverage Score -1.6665704857737749\n",
      "Average loss: 0.0016786765999212447\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 24000\tAverage Score -1.666066163629602\n",
      "Average loss: 0.0017499146504431134\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 24100\tAverage Score -1.665544189290171\n",
      "Average loss: 0.0012244987107502918\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 24200\tAverage Score -1.665005226294221\n",
      "Average loss: 0.0010941839783400712\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 24300\tAverage Score -1.6641742991413957\n",
      "Average loss: 0.001078097292338498\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 24400\tAverage Score -1.6637936915794787\n",
      "Average loss: 0.0008552847349266005\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 24500\tAverage Score -1.6630026575135783\n",
      "Average loss: 0.0019054903047314535\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 24600\tAverage Score -1.6625851250977333\n",
      "Average loss: 0.000995104358298704\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 24700\tAverage Score -1.6619616492896057\n",
      "Average loss: 0.0011515217322287047\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 24800\tAverage Score -1.6615557109221153\n",
      "Average loss: 0.0006222286756383255\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 24900\tAverage Score -1.6608275329764148\n",
      "Average loss: 0.000922077075099676\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 25000\tAverage Score -1.660728892660162\n",
      "Average loss: 0.0011871044931467622\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 25100\tAverage Score -1.6604166847080304\n",
      "Average loss: 0.0012494793692086306\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 25200\tAverage Score -1.659530477339331\n",
      "Average loss: 0.0011636706153189556\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 25300\tAverage Score -1.6588808991567212\n",
      "Average loss: 0.0007905842765467241\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 25400\tAverage Score -1.658146758826335\n",
      "Average loss: 0.00123923811608822\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 25500\tAverage Score -1.6575620643265205\n",
      "Average loss: 0.001027002662885934\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 25600\tAverage Score -1.656491512114532\n",
      "Average loss: 0.0006173152133770701\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 25700\tAverage Score -1.6558071764466267\n",
      "Average loss: 0.0010940114763798193\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 25800\tAverage Score -1.6552606108658126\n",
      "Average loss: 0.0008979197138816946\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 25900\tAverage Score -1.6551458568616704\n",
      "Average loss: 0.0007985189266037195\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 26000\tAverage Score -1.6547130687456966\n",
      "Average loss: 0.0008586941823725485\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 26100\tAverage Score -1.6540950054290997\n",
      "Average loss: 0.0008970007882453501\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 26200\tAverage Score -1.6538598666539623\n",
      "Average loss: 0.0008613999881264237\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 26300\tAverage Score -1.6534329578217768\n",
      "Average loss: 0.0010645952263277853\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 26400\tAverage Score -1.6527665631469985\n",
      "Average loss: 0.0011115379925791381\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 26500\tAverage Score -1.6524896718897983\n",
      "Average loss: 0.0018710427878734965\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 26600\tAverage Score -1.6514060793793204\n",
      "Average loss: 0.001036322957952507\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 26700\tAverage Score -1.6505263291766346\n",
      "Average loss: 0.0011059760549364404\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 26800\tAverage Score -1.649631946323167\n",
      "Average loss: 0.000783905822027009\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 26900\tAverage Score -1.6489299753913336\n",
      "Average loss: 0.0007009411961512847\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 27000\tAverage Score -1.6487690643356765\n",
      "Average loss: 0.0013242775263885658\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 27100\tAverage Score -1.6483800345672392\n",
      "Average loss: 0.001004866196308285\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 27200\tAverage Score -1.6481876986608215\n",
      "Average loss: 0.001290659556010117\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 27300\tAverage Score -1.6475177868365365\n",
      "Average loss: 0.0009430900405277498\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 27400\tAverage Score -1.6466890451442908\n",
      "Average loss: 0.0010811372550152657\tRandom action sample probability: 0.01\n",
      "\n",
      "Episode 27429\tEpisode Score -4.99999999999999\tLast iters score -2.5249999999999972"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-368-ee7352e08b6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m111\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-367-7bfe6b27cce4>\u001b[0m in \u001b[0;36mtraining_loop\u001b[1;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-366-d93af2dda908>\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state, eps)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0maction_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-362-ebfac80fb241>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;31m#print(x.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = training_loop()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Epsiode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-45f2a19e878e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rbg_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size = 196, action_size = 5, seed=555555)\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "for i in range(3):\n",
    "    state = env.reset()\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    for j in range(200):\n",
    "        sleep(0.2)\n",
    "        action = agent.act(state)\n",
    "        img.set_data(env.render(mode='rbg_array'))\n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  }
 ]
}